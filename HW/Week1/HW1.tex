\documentclass{article} % Uses 10pt

\usepackage{fancyhdr}
\usepackage{helvet}
\usepackage{url}
\usepackage{amsmath,amssymb}
\textwidth6.5in
\textheight=9.7in
\setlength{\topmargin}{-0.3in}
\addtolength{\topmargin}{-\headheight}
\addtolength{\topmargin}{-\headsep}
\headsep = 20pt

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\renewcommand{\baselinestretch}{1.4}

\setlength{\oddsidemargin}{0in}

\oddsidemargin  0.0in \evensidemargin 0.0in

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.0pt}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\rfoot{}
\cfoot{}


\newcommand{\sectionname}[1]{\vspace{0.5cm} \noindent {\bf #1}}

\begin{document}

\begin{center}
  \textbf{\large Stat 516, Homework 1}
\end{center}
\sectionname{Due date}:  Thursday, October 5, at the beginning of class.

\begin{enumerate}
\item {\it Gamma-Poisson mixture}

  Recall that if $Z \sim \text{Poisson}(\lambda)$, then $\text{E}(Z) =
  \text{var}(Z) = \lambda$, where $\lambda>0$ is the intensity
  parameter.  This property of the Poisson distribution is restrictive
  since often in practice {\it excess-Poisson variation} is
  encountered, i.e.~the variance of observed count data exceeds the
  mean. One possible solution to this discrepancy is to use a
  Gamma-Poisson mixture, which is generated by first randomly drawing
  $X \sim \text{Gamma}(\alpha, \beta)$, where $\alpha, \beta > 0$ (the
  parameterization is such that that $\text{E}(X) = \alpha/ \beta$ and
  $\text{var}(X) = \alpha/ \beta^2$) and then generating $Y \mid X
  \sim \text{Poisson}(X)$.
\begin{enumerate}
\item Derive $\text{E}(Y)$ and $\text{var}(Y)$ and show
  that $\text{E}(Y) < \text{var}(Y)$.

\begin{flalign*}
E[Y] & = E[E(Y|X)] \\
& = E[X] \\
& = \frac{\alpha}{\beta}  \\
~ \\
Var[Y] & = E[Var(Y|X)] + Var[E(N|X)] \\
& = \frac{\alpha}{\beta} + \frac{\alpha}{\beta^2} \\
& = \frac{\beta \alpha + \alpha}{\beta^2}\\
& = \frac{\alpha (\beta + 1)}{\beta^2} \\
~ \\
E[Y] & < Var[Y] \\
\frac{\alpha}{\beta} & < \frac{\alpha(\beta + 1)}{\beta^2} \\
\alpha \beta & < \alpha(\beta + 1) \\
\beta & < \beta + 1 \\
0 & < 1
\end{flalign*}

\item Derive the  marginal distribution of $Y$.
\end{enumerate}

\begin{flalign*}
  P(Y=k | \alpha, \beta) & = \int_{0}^{\infty}
  \frac{\beta^{\alpha}}{\Gamma(\alpha)} {\lambda}^{\alpha-1}
  e^{-\beta \lambda}
  \frac{\lambda^{k} e^{-\lambda}}{k!} d\lambda \\
  & = \frac{\beta^{\alpha}}{\Gamma(\alpha) k!}
  \int_{0}^{\infty} {\lambda}^{\alpha + k -1}
  e^{-\lambda(\beta + 1)} d\lambda
\end{flalign*}

\item {\it Br\'{e}maud, 1.7.1.}

Let $X_1$ and $X_2$ be two independent random variables taking their
values in $\{1,2,\dots,N\}$, and uniformly distributed, that is,
$\Pr(X_i = k) = \Pr(X_2 = k) = \frac{1}{N}$, $1\le k\le N$.
Compute $\mbox{E}[X_1\,|\, \max\{X_1,X_2\}]$.

\begin{flalign*}
Z = max\{X_1 , X_2 \} \\
P(X_1 > Z| Z) & = 0 \\
P(X_1 = Z| Z) & = \frac{Z}{2Z - 1} \\
~ \\
V = \{1, 2, ... Z-1\} \\
v \in V \\
P(X_1 = v| Z) & = \frac{1 - P(X_1 = Z| Z)}{Z-1} \\
P(X_1 = v| Z) & = \frac{1 - \frac{Z}{2Z - 1}}{Z-1} \\
& = \frac{\frac{2Z - 1}{2Z - 1} - \frac{Z}{2Z - 1}}{Z-1} \\
& = \frac{\frac{Z - 1}{2Z - 1}}{Z-1} \\
& = \frac{Z - 1}{(Z-1)(2Z - 1)} \\
& = \frac{1}{2Z-1} \\
~ \\
E[X_1| Z] & = \sum_{x=1}^Z p(x) x\\
&= \frac{Z^2}{2Z - 1}\sum_{x=1}^{Z-1} \frac{x}{2Z-1} \\
&= \frac{Z^2 + \sum_{x=1}^{Z-1} x}{2Z - 1}
\end{flalign*}

\item {\it Br\'{e}maud, 1.4.1:~Exponential races}

 Let $X_i$, $i=1,\dots,n$, be independent exponential
random variables with intensities/rates $\lambda_i >0$, $i=1,\dots,n$
respectively. Let
$Z = \min(X_1,\dots,X_n)$ and $\displaystyle J = \argmin_j X_j$. In other
words, $J$ is a random index corresponding to $Z = X_j$; $J$ is well defined,
because the probability that one or more $X_i$'s attain the same value is $0$.
Show that $Z$ and $J$ are independent, and give their respective
distributions. \\*[0.1cm]
{\it Hint:} Start with $\text{Pr}(J=k,Z \ge t)$, then condition on the random
variable $Z$. You may find Br\'{e}maud's Exercise 4.2
({\it Freezing a Random Variable}) useful, if you keep in mind that for any
event $A$, $\text{Pr}(A) = \text{E}(1_A)$. This exercise states that:

Let $X_1,...,X_n$ be independent random variables with respective p.d.f.'s
$f_1$,...,$f_n$. Show that
$$\mbox{E}[ g(X_1,...,X_n)] = \int_{-\infty}^{\infty} \mbox{E}[g(y,X_2,...,X_n)] f_1(y) ~dy$$
and that
$$\Pr( X_1 \leq X_2,...,X_1 \leq X_n,X_1 \leq x) = \int_{-\infty}^x \Pr(X_2 \geq y) \times ...\times \Pr(X_n \geq y) f_1(y)~dy.$$

\item {\it Br\'{e}maud, 1.8.1: Applying the SLLN}
  \begin{enumerate}
  \item State Kolmogorov's Strong Law of Large Numbers (SLLN).
  \item Let $S_1,S_2,\dots$ be a sequence of independent and
    identically distributed (i.i.d.) random variables, with
    $\Pr(0<S_1<\infty)=1$ and $\mathbb{E}[S_1]<\infty$.  For $t\ge 0$,
    let $N_t=\sum_{n=1}^\infty 1_{(0,t]}(T_n)$, where
    $T_n=S_1+\dots+S_n$.  Prove that, almost surely,
    \[
    \lim_{t\to\infty} \frac{N_t}{t} = \frac{1}{\mathbb{E}[S_1]}.
    \]
  \end{enumerate}

\end{enumerate}

\end{document}

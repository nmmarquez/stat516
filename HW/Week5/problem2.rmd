---
title: "HW 5 Problem 2"
output:
  pdf_document: default
  html_document: default
---

#2)

```{r Read data}
# Read in and deal with formatting
snoqualmie <- readLines("/home/nyanyan/Documents/Classes/stat516/HW/Week5/snoqualmie.txt")
snoqualmie <- gsub(" {2,}"," ",snoqualmie)
snoqualmie <- strsplit(snoqualmie," ")
snoqualmie <- lapply(snoqualmie,function(yr){
  return(as.numeric(yr[-1]))
})

# Get June and make binary
june <- lapply(snoqualmie,function(yr){
  j1 <- 3*31+30+28+1 + ifelse( length(yr) == 366, 1, 0)
  j <- yr[j1:(j1+29)]
  return(ifelse(j > 0, 1, 0))
})
june <- do.call(rbind,june)
```

a)

We need to count four kinds of events, 0 -> 0, 0 -> 1, 1 -> 0, 1 -> 1.

```{r 2a}
n11 <- sum(june[,1:29] == 0 & june[,2:30] == 0)
n12 <- sum(june[,1:29] == 0 & june[,2:30] == 1)
n21 <- sum(june[,1:29] == 1 & june[,2:30] == 0)
n22 <- sum(june[,1:29] == 1 & june[,2:30] == 1)

n <- n11 + n12 + n21 + n22

p_hat_11 <- n11 / (n11 + n12)
p_hat_12 <- n12 / (n11 + n12)
p_hat_21 <- n21 / (n21 + n22)
p_hat_22 <- n22 / (n21 + n22)

P_hat <- rbind(c(p_hat_11,p_hat_12),c(p_hat_21,p_hat_22))
colnames(P_hat) <- c("dry","wet")
row.names(P_hat) <- c("dry","wet")
```

All of this is to say that our MLE of $P$ is:

```{R P_hat}
P_hat
```

We get the 95% asymptotic confidnence intervals by noting that
$$
\frac{\hat{p_{ij}} - p_{ij}}{\sqrt{p_{ij}(1 - p_{ij})/n\pi_i}} \underset{d}\rightarrow N(0,1)
$$
Meaning that our 95% asymptotic CI is
$$
\Bigg[\ \hat{p_{ij}} - 1.96 \times \sqrt{\frac{\hat{p_{ij}}(1 - \hat{p_{ij}})}{n\hat{\pi_i}}},\hat{p_{ij}} + 1.96 \times \sqrt{\frac{\hat{p_{ij}}(1 - \hat{p_{ij}})}{n\hat{\pi_i}}}\ \Bigg]
$$
```{r Confidence intervals}
pi_hat_1 <- sum(june == 0)/(sum(june == 0) + sum(june == 1))
pi_hat_2 <- sum(june == 1)/(sum(june == 0) + sum(june == 1))

ci_p_12 <- c(p_hat_12 - 1.96 * sqrt(p_hat_12 * (1 - p_hat_12) / (n * pi_hat_1)),
             p_hat_12 + 1.96 * sqrt(p_hat_12 * (1 - p_hat_12) / (n * pi_hat_1)))

ci_p_21 <- c(p_hat_21 - 1.96 * sqrt(p_hat_21 * (1 - p_hat_21) / (n * pi_hat_2)),
             p_hat_21 + 1.96 * sqrt(p_hat_21 * (1 - p_hat_21) / (n * pi_hat_2)))
```
So our 95% asymptotic CI for $p_12$ is,
```{r CI p12}
ci_p_12
```
and our 95% asymptotic CI for $p_21$ is:
```{r CI p21}
ci_p_21
```

b)

A Uniform(0,1) prior is a Beta(1,1) prior, making the posterior distribution beta as well (since the Beta is the 2D Dirichlet distribution, and the Dirichlet is the conjugate prior for unrestricted transition matrices).

Specifically, with a Beta(1,1) prior, the posterior $p_{ij}\ |\ n_{ij}, n_{ii} \sim Beta(1\ +\ n_{ij},\ 1\ +\ n_{ii})$

This means we can get our posterior medians and confidence intervals easily, as:
```{r Posterior on pij}
posterior_median_p_12 <- qbeta(0.5,1+n12,1+n11)
posterior_median_p_21 <- qbeta(0.5,1+n21,1+n22)

posterior_ci_p_12 <- qbeta(c(0.025,0.975),1+n12,1+n11)
posterior_ci_p_21 <- qbeta(c(0.025,0.975),1+n21,1+n22)

posterior_median_p_12
posterior_median_p_21

posterior_ci_p_12
posterior_ci_p_21
```


c)

The independence model is one with the following transition matrix:
$$
\begin{bmatrix}
p_{11} & p_{12} \\
p_{11} & p_{12}
\end{bmatrix}
$$
That is, both rows are identical, and there is only one free parameter in the model. 
We note that in the indendence model, $p_{11}$ is the probability of transitioning to a dry day (given a wet or a dry day) and $p_{12}$ is the probability of transitioning to a wet day (given a dry or wet day).

It is then clear that the MLE of,$p_{11}$, $\hat{p^*_{11}} = \frac{n_{11}+n_{21}}{n}$.
Similarly the MLE of,$p_{12}$, $\hat{p^*_{12}} = \frac{n_{12}+n_{22}}{n}$.

The likelihood ratio test here is distriuted $\chi^2(df=1)$, as the independent model is found by restricting the alternative model to have $p_{21} = p_{11}$ and $p_{22} = p_{12}$

```{r LRT vs independence}
p_hat_star_11 <- (n11 + n21)/n
p_hat_star_12 <- (n21 + n22)/n
lnL_independence <- (n11+n21)*log(p_hat_star_11) + (n21+n22)*log(p_hat_star_12)
lnL_Markov <- n11*log(p_hat_11) + n12*log(p_hat_12) + n21*log(p_hat_21) + n22*log(p_hat_22)

LR_vs_independence <- 2 * (lnL_Markov - lnL_independence)

LR_vs_independence

pchisq(LR_vs_independence,1,lower.tail=FALSE)

```

To get the Bayes Factor, we note that the marginal likelihood of the independence model, $Pr(\textbf{y}\ |\ H_o)$ is given by

$$
\begin{aligned}
\int_0^1Pr(\textbf{y}\ |\ p_{11},\ H_0)\ p(p_{11}\ |\ H_0) =& \frac{\Gamma(1 + 1)}{\Gamma(1)\Gamma(1)}\int_0^1(p_{11})^{n_{11}+n_{21}+1-1}(1 - p_{11})^{n_{12}+n_{22}+1-1}dp_{11}\\
=&\frac{\Gamma(1 + 1)}{\Gamma(1)\Gamma(1)}\frac{\Gamma(n_{11} +n_{21} + 1)\Gamma(n_{12} +n_{22} + 1)}{\Gamma(n+1+1)}\\
=&\frac{\Gamma(n_{11} +n_{21} + 1)\Gamma(n_{12} +n_{22} + 1)}{\Gamma(n+1+1)}
\end{aligned}
$$

The marginal likelihood for the Markov model, $Pr(\textbf{y}\ |\ H_1)$ is given by

$$
\begin{aligned}
\int_0^1Pr(\textbf{y}\ |\ p,\ H_1)\ p(p_{11}, p_{21}\ |\ H_1) =& 
\frac{\Gamma(1 + 1)}{\Gamma(1)\Gamma(1)}\frac{\Gamma(1 + 1)}{\Gamma(1)\Gamma(1)}\\
&\times\int_0^1 \int_0^1(p_{11})^{n_{11}+1-1}(1 - p_{11})^{n_{12}+1-1}(p_{21})^{n_{21}+1-1}(1 - p_{21})^{n_{22}+1-1}dp_{11}dp_{21}\\
=&\frac{\Gamma(n_{11} + 1)\Gamma(n_{12} + 1)}{\Gamma(n_{11}+n_{12}+1+1)}
\frac{\Gamma(n_{21} + 1)\Gamma(n_{22} + 1)}{\Gamma(n_{21}+n_{22}+1+1)}\\
\end{aligned}
$$


```{r Bayes Factor versus independence}
# We tranform to the log-scale temporarily to avoid numerical issues
marginal_indpendence <- exp(lgamma(n11+n21+1) + lgamma(n12+n22+1) - lgamma(n+1))
marginal_Markov <- exp(lgamma(n11+1) + lgamma(n12+1) + lgamma(n21+1) + 
                         lgamma(n22+1) - lgamma(n11+n12+2) - lgamma(n21+n22+2))

BF_vs_independence <- marginal_Markov/marginal_indpendence

BF_vs_independence
```

d)

First, we loop over all the years to record the year-specific TPMs for June, so we can plot the $logit(p_{ij})$. Then we loop over the years again to calculate the likelihood ratio.

To conduct the LRT, we note that the test statistic is distributed $\chi^2(df=70)$, since there are $36*2=72$ free parameters in the model where each year has its own matrix, and $2$ in the simpler model.

```{r One TPM per year}
# Get all the pij for each year, stored in a matrix, such that in the matrix rows 1:4 are p11, p12, p21, and p22, and the columns are years 1:36
yearly_P <- apply(june,1,function(june_year){
  n11 <- sum(june_year[1:29] == 0 & june_year[2:30] == 0)
  n12 <- sum(june_year[1:29] == 0 & june_year[2:30] == 1)
  n21 <- sum(june_year[1:29] == 1 & june_year[2:30] == 0)
  n22 <- sum(june_year[1:29] == 1 & june_year[2:30] == 1)
  
  n <- n11 + n12 + n21 + n22
  
  p_hat_11 <- n11 / (n11 + n12)
  p_hat_12 <- n12 / (n11 + n12)
  p_hat_21 <- n21 / (n21 + n22)
  p_hat_22 <- n22 / (n21 + n22)
  
  return(c(p_hat_11,p_hat_12,p_hat_21,p_hat_22))
  
})

# Calculate and plot the logit pij as requested
logit_p12 <- log(yearly_P[2,]/(1-yearly_P[2,]))
logit_p21 <- log(yearly_P[3,]/(1-yearly_P[3,]))

plot(logit_p12,xlab="year",ylab="logit(p_ij)",pch=1,ylim=c(min(c(logit_p12,logit_p21)),max(c(logit_p12,logit_p21))))
points(logit_p21,pch=16)
legend("topright",legend=c("p_12","p_21"),pch=c(1,16),border=NA,bty="n")

# Carry out the LRT
yearly_lnL_vector <- apply(june,1,function(june_year){
  n11 <- sum(june_year[1:29] == 0 & june_year[2:30] == 0)
  n12 <- sum(june_year[1:29] == 0 & june_year[2:30] == 1)
  n21 <- sum(june_year[1:29] == 1 & june_year[2:30] == 0)
  n22 <- sum(june_year[1:29] == 1 & june_year[2:30] == 1)
  
  n <- n11 + n12 + n21 + n22
  
  p_hat_11 <- n11 / (n11 + n12)
  p_hat_12 <- n12 / (n11 + n12)
  p_hat_21 <- n21 / (n21 + n22)
  p_hat_22 <- n22 / (n21 + n22)
  
  lnL <- n11*log(p_hat_11) + n12*log(p_hat_12) + n21*log(p_hat_21) + n22*log(p_hat_22)
  
  return(lnL)
  
})

# The log-likelihood for the "every year to itself" model is the sum of the log-likelihoods of each year
lnL_yearly <- sum(yearly_lnL_vector)

LR_vs_yearly <- 2 * (lnL_yearly - lnL_Markov)

pchisq(LR_vs_yearly,36*2-2,lower.tail=FALSE)

```

Lastly, we loop over all the years one last time so we can get the marginal likelihood of the year-specific model. Since all the years are independent, our 72-dimensional integral is easy to decompose into 36 that look a lot like the ones we calculated above.

```{r Bayes Factor versus per-year model}
yearly_lnL_marginal_likelihood <- apply(june,1,function(june_year){
  n11 <- sum(june_year[1:29] == 0 & june_year[2:30] == 0)
  n12 <- sum(june_year[1:29] == 0 & june_year[2:30] == 1)
  n21 <- sum(june_year[1:29] == 1 & june_year[2:30] == 0)
  n22 <- sum(june_year[1:29] == 1 & june_year[2:30] == 1)
  
  n <- n11 + n12 + n21 + n22
  
  p_hat_11 <- n11 / (n11 + n12)
  p_hat_12 <- n12 / (n11 + n12)
  p_hat_21 <- n21 / (n21 + n22)
  p_hat_22 <- n22 / (n21 + n22)
  
  this_year_log_marginal <- lgamma(n11+1) + lgamma(n12+1) + lgamma(n21+1) + lgamma(n22+1) - lgamma(n11+n12+2) - lgamma(n21+n22+2)

  return(this_year_log_marginal)
  
})

marginal_yearly <- exp(sum(yearly_lnL_marginal_likelihood))

BF_vs_yearly <- marginal_yearly/marginal_Markov

BF_vs_yearly

```

On the basis of the likelihood test, we would reject the simpler model where there is a single TPM shared between all Junes in all years in favor of each June getting its own TPM.

On the basis of the Bayes Factor, we would not reject the single TPM for all Junes. In fact, since the simpler model has a marginal likelihood on the order of $10^{16}$ times greater than the TPM-per-yaer model, we would be inclined to view this as strong evidence in favor of the simpler model.

On the one hand, our plot shows a great deal of year-to-year heterogeneity in the transition matrix parameters. The LRT sees this as very important. On the other hand, each of those is based on 29 transitions, from which 2 free parameters are being estimated, so it feels like we might be overfitting a bit, and the Bayes Factor punishes us more heavily here for the high dimensionality of our more complicated model.

Our LRT returns a p-value of $\approx 0.028$, reasonable evidence against the simple model. But our Bayes Factors says that the more complicated model is $\approx 4.39\times10^{-17}$ as probable as our simple model, which is a stronger preference in the opposite direction. It does not seem unreasonable to conclude that a single TPM is the better choice.

<!-- #1) -->

<!-- d) -->

<!-- ```{r find Gamma parameters} -->
<!-- library(nloptr) -->

<!-- fnOpt <- function(par) { -->
<!--   a <- par[1] -->
<!--   b <- par[2] -->
<!--   return( ( ((0.1 - qgamma(0.05,a,b))/0.1)^2 + ((10 - qgamma(0.95,a,b))/10)^2) ) -->
<!-- } -->

<!--   # Some experimentation shows that the algorithm NLOPT_LN_PRAXIS, NLOPT_LN_BOBYQA, and NLOPT_LN_COBYLA are relatively good at not choking and shooting a rate to 0 -->
<!--   # Further experimentation suggested BOBYQA yielded the fastest evaluations and highest likelihoods on termination -->
<!--   nloptr.control <- list("algorithm"="NLOPT_LN_BOBYQA", "maxeval"="1000000", "ftol_rel"=.Machine$double.eps^0.5) -->

<!-- opt <- nloptr(c(1,1),fnOpt,lb=c(0,0),opts=nloptr.control) -->

<!-- opt$solution -->

<!-- qgamma(c(0.05,0.95),opt$solution[1],opt$solution[2]) -->

<!-- E <- 0.25 -->
<!-- y <- 4 -->

<!-- posterior_samples <- rgamma(1e5,shape=opt$solution[1]+y,rate=opt$solution[2]+E) -->

<!-- hist(posterior_samples,main="Samples from Pr(theta | y)",xlab="theta",freq=FALSE) -->

<!-- posterior_ci_theta <- qgamma(c(0.025,0.975),shape=opt$solution[1]+y,rate=opt$solution[2]+E) -->
<!-- posterior_ci_theta -->
<!-- ``` -->

<!-- e) -->

<!-- Yes, there is evidence of excess risk for these data. The Bayesian analysis yields a 95% CI that doesn't come anywhere near 1, and the asymptotic 95% CI of the likelihood analysis... -->
